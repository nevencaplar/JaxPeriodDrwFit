{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your runtime has 270.2 gigabytes of available RAM\n",
      "\n",
      "You are using a high-RAM runtime!\n"
     ]
    }
   ],
   "source": [
    "from psutil import virtual_memory\n",
    "ram_gb = virtual_memory().total / 1e9\n",
    "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
    "\n",
    "if ram_gb < 20:\n",
    "  print('Not using a high-RAM runtime')\n",
    "else:\n",
    "  print('You are using a high-RAM runtime!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initialized persistent compilation cache at ./cache_min_example\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eztao version: 0.4.1\n",
      "jax version: 0.4.18\n",
      "tinygp version: 0.2.4\n",
      "cpu/gpu: gpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.scipy.optimize as jsco\n",
    "from jax.lib import xla_bridge\n",
    "from jax._src import compilation_cache as cc\n",
    "\n",
    "import tinygp\n",
    "from tinygp import GaussianProcess, kernels\n",
    "from tinygp.kernels import quasisep\n",
    "\n",
    "import eztao\n",
    "from eztao.carma import DRW_term\n",
    "from eztao.ts import gpSimRand\n",
    "\n",
    "import logging\n",
    "\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "cc.initialize_cache(\"./cache_min_example\")\n",
    "jax.config.update(\"jax_persistent_cache_min_compile_time_secs\", 0.1)\n",
    "\n",
    "print(\"eztao version: \" + eztao.__version__)\n",
    "\n",
    "print(\"jax version: \" + jax.__version__)\n",
    "print(\"tinygp version: \" + tinygp.__version__)\n",
    "\n",
    "print(\"cpu/gpu: \" + str(xla_bridge.get_backend().platform))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_batch = []\n",
    "y_batch = []\n",
    "yerr_batch = []\n",
    "len_lc = []\n",
    "\n",
    "# Create 10 lightcurves with different length\n",
    "for i in range(10):  \n",
    "  amp = 0.2\n",
    "  tau = 100\n",
    "  DRW_kernel = DRW_term(np.log(amp), np.log(tau))\n",
    "  t, y_drw, yerr = gpSimRand(DRW_kernel, 10, 365*10, 203+i)\n",
    "\n",
    "  # This adds periodic component to the drw process\n",
    "  y = y_drw + 0.2* np.sin(t/100)\n",
    "\n",
    "  t_batch.append(t)\n",
    "  y_batch.append(y)\n",
    "  yerr_batch.append(yerr)\n",
    "  len_lc.append(len(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-20 11:12:17.934670: W external/xla/xla/service/gpu/nvptx_compiler.cc:703] The NVIDIA driver's CUDA version is 12.1 which is older than the ptxas CUDA version (12.2.140). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n",
      "INFO:jax._src.compilation_cache:Writing jit_neg_log_likelihood_float_jit to persistent compilation cache with key jit_neg_log_likelihood_float_jit-dee2dc0c8308575790c32afdc7b585cbafee9b791cf75b69db235db5e45815ae.\n",
      "INFO:jax._src.compiler:Not writing persistent cache entry for 'jit_neg_log_likelihood_float_jit_quasisep' because it uses host callbacks (e.g. from jax.debug.print or breakpoint)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Array(1190.59536823, dtype=float64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a single simple testing lightcurve\n",
    "t, y, yerr = np.arange(0, 1000, 1), np.sin(np.arange(0, 1000, 1)), np.ones(1000)\n",
    "\n",
    "theta_init_float = [np.log10(100), np.log10(0.25),np.log10(1.2), np.log10(4.3)]\n",
    "\n",
    "def build_exp_gp_kernels(theta_float, t, y, yerr):\n",
    "    \"\"\"Build an Gaussian proccess that is only exp\n",
    "    \"\"\"\n",
    "\n",
    "    log_drw_scale = theta_float[0]\n",
    "    exp_kernel = kernels.Exp(scale=10**log_drw_scale)\n",
    "    return GaussianProcess(exp_kernel, t, diag=yerr, mean=np.mean(y))\n",
    "\n",
    "def build_exp_gp_quasi(theta_float, t, y, yerr):\n",
    "    \"\"\"Build an Gaussian proccess that is only exp, but quasiseparable\n",
    "    \"\"\"\n",
    "\n",
    "    log_drw_scale = theta_float[0]\n",
    "    exp_kernel = quasisep.Exp(scale=10**log_drw_scale)\n",
    "    return GaussianProcess(exp_kernel, t, diag=yerr, mean=np.mean(y))\n",
    "\n",
    "def build_gp_float(theta_float, t, y, yerr):\n",
    "    \"\"\"Build an Gaussian proccess that is a combination of exp and periodic\n",
    "    \"\"\"\n",
    "    log_drw_scale = theta_float[0]\n",
    "    log_drw_amp = theta_float[1]\n",
    "    log_per_scale = theta_float[2]\n",
    "    log_per_amp = theta_float[3]\n",
    "\n",
    "    sigma_drw = 10**log_drw_amp\n",
    "    sigma_per = 10**log_per_amp\n",
    "\n",
    "    exp_kernel = kernels.Exp(scale=10**log_drw_scale)\n",
    "    periodic_kernel = kernels.Cosine(scale=10**(log_per_scale))\n",
    "\n",
    "    kernel = sigma_drw * exp_kernel + sigma_per * periodic_kernel\n",
    "\n",
    "    return GaussianProcess(kernel, t, diag=yerr, mean=np.mean(y))\n",
    "\n",
    "def build_gp_float_quasisep(theta_float, t, y, yerr):\n",
    "    \"\"\"Build an Gaussian proccess that is a combination of exp and periodic\n",
    "    \"\"\"\n",
    "    log_drw_scale = theta_float[0]\n",
    "    log_drw_amp = theta_float[1]\n",
    "    log_per_scale = theta_float[2]\n",
    "    log_per_amp = theta_float[3]\n",
    "\n",
    "    exp_kernel = quasisep.Exp(\n",
    "        scale=10**log_drw_scale, sigma=10**log_drw_amp\n",
    "    )\n",
    "\n",
    "    periodic_kernel = (\n",
    "        quasisep.Cosine(\n",
    "        scale=10**(log_per_scale),\n",
    "        sigma=10**(log_per_amp),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    kernel = exp_kernel + periodic_kernel\n",
    "\n",
    "    return GaussianProcess(kernel, t, diag=yerr, mean=np.mean(y))\n",
    "\n",
    "\"\"\"\n",
    "@jax.jit\n",
    "def neg_log_likelihood_kernels(theta, t, y, yerr):\n",
    "    gp = build_exp_gp_kernels(theta, t, y, yerr)\n",
    "    return -gp.log_probability(y)\n",
    "\n",
    "@jax.jit\n",
    "def neg_log_likelihood_quasi(theta, t, y, yerr):\n",
    "    gp = build_exp_gp_quasi(theta, t, y, yerr)\n",
    "    return -gp.log_probability(y)\n",
    "\"\"\"\n",
    "\n",
    "def neg_log_likelihood_float(theta_float, t, y, yerr):\n",
    "    gp = build_gp_float(theta_float, t, y, yerr)\n",
    "    return -gp.log_probability(y)\n",
    "\n",
    "def neg_log_likelihood_float_quasisep(theta_float, t, y, yerr):\n",
    "    gp = build_gp_float_quasisep(theta_float, t, y, yerr)\n",
    "    return -gp.log_probability(y)\n",
    "\n",
    "@jax.jit\n",
    "def neg_log_likelihood_float_jit(theta_float, t, y, yerr):\n",
    "    gp = build_gp_float(theta_float, t, y, yerr)\n",
    "    return -gp.log_probability(y)\n",
    "\n",
    "@jax.jit\n",
    "def neg_log_likelihood_float_jit_quasisep(theta_float, t, y, yerr):\n",
    "    gp = build_gp_float_quasisep(theta_float, t, y, yerr)\n",
    "    return -gp.log_probability(y)\n",
    "\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logging.debug(\"test\")\n",
    "\n",
    "# both neg_log_likelihood are written\n",
    "neg_log_likelihood_float_jit(theta_init_float, t, y, yerr)\n",
    "neg_log_likelihood_float_jit_quasisep(theta_init_float, t, y, yerr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(1190.59536823, dtype=float64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_log_likelihood_float_jit(theta_init_float, t, y, yerr)\n",
    "neg_log_likelihood_float_jit_quasisep(theta_init_float, t, y, yerr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jsoln_jax_ty(t,y,yerr):\n",
    "    # via jax implementation of scipy minimize\n",
    "    # remove jnp.array, i.e., demand that inputs are jax arrays?\n",
    "    jsoln = jsco.minimize(neg_log_likelihood_float, x0=jnp.array(theta_init_float), method=\"bfgs\", args=(jnp.array(t), jnp.array(y), jnp.array(yerr)))\n",
    "    return jsoln.fun\n",
    "\n",
    "def jsoln_jax_ty_quasisep(t,y,yerr):\n",
    "    # via jax implementation of scipy minimize\n",
    "    # remove jnp.array, i.e., demand that inputs are jax arrays?\n",
    "    jsoln = jsco.minimize(neg_log_likelihood_float_quasisep, x0=jnp.array(theta_init_float), method=\"bfgs\", args=(jnp.array(t), jnp.array(y), jnp.array(yerr)))\n",
    "    return jsoln.fun\n",
    "\n",
    "# what if I use already jitted likelihoods?\n",
    "def jsoln_jax_ty_jit(t,y,yerr):\n",
    "    # via jax implementation of scipy minimize\n",
    "    # remove jnp.array, i.e., demand that inputs are jax arrays?\n",
    "    jsoln = jsco.minimize(neg_log_likelihood_float_jit, x0=jnp.array(theta_init_float), method=\"bfgs\", args=(jnp.array(t), jnp.array(y), jnp.array(yerr)))\n",
    "    return jsoln.fun\n",
    "\n",
    "def jsoln_jax_ty_jit_quasisep(t,y,yerr):\n",
    "    # via jax implementation of scipy minimize\n",
    "    # remove jnp.array, i.e., demand that inputs are jax arrays?\n",
    "    jsoln = jsco.minimize(neg_log_likelihood_float_jit_quasisep, x0=jnp.array(theta_init_float), method=\"bfgs\", args=(jnp.array(t), jnp.array(y), jnp.array(yerr)))\n",
    "    return jsoln.fun\n",
    "\n",
    "jsoln_jax_ty_gpu = jax.jit(jsoln_jax_ty, backend=\"gpu\")\n",
    "jsoln_jax_ty_quasisep_gpu = jax.jit(jsoln_jax_ty_quasisep, backend=\"gpu\")\n",
    "jsoln_jax_ty_jit_gpu = jax.jit(jsoln_jax_ty_jit, backend=\"gpu\")\n",
    "jsoln_jax_ty_jit_quasisep_gpu = jax.jit(jsoln_jax_ty_jit_quasisep, backend=\"gpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "XlaRuntimeError",
     "evalue": "INTERNAL: cuSolver internal error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXlaRuntimeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:1\u001b[0m\n",
      "    \u001b[0;31m[... skipping hidden 14 frame]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/jax-gpu/lib/python3.10/site-packages/jax/_src/compiler.py:256\u001b[0m, in \u001b[0;36mbackend_compile\u001b[0;34m(backend, module, options, host_callbacks)\u001b[0m\n\u001b[1;32m    251\u001b[0m   \u001b[39mreturn\u001b[39;00m backend\u001b[39m.\u001b[39mcompile(built_c, compile_options\u001b[39m=\u001b[39moptions,\n\u001b[1;32m    252\u001b[0m                          host_callbacks\u001b[39m=\u001b[39mhost_callbacks)\n\u001b[1;32m    253\u001b[0m \u001b[39m# Some backends don't have `host_callbacks` option yet\u001b[39;00m\n\u001b[1;32m    254\u001b[0m \u001b[39m# TODO(sharadmv): remove this fallback when all backends allow `compile`\u001b[39;00m\n\u001b[1;32m    255\u001b[0m \u001b[39m# to take in `host_callbacks`\u001b[39;00m\n\u001b[0;32m--> 256\u001b[0m \u001b[39mreturn\u001b[39;00m backend\u001b[39m.\u001b[39;49mcompile(built_c, compile_options\u001b[39m=\u001b[39;49moptions)\n",
      "\u001b[0;31mXlaRuntimeError\u001b[0m: INTERNAL: cuSolver internal error"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:jax._src.compiler:Not writing persistent cache entry for 'jit_jsoln_jax_ty_quasisep' because it uses host callbacks (e.g. from jax.debug.print or breakpoint)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.69 s, sys: 276 ms, total: 7.96 s\n",
      "Wall time: 9.42 s\n"
     ]
    },
    {
     "ename": "XlaRuntimeError",
     "evalue": "INTERNAL: cuSolver internal error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXlaRuntimeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:1\u001b[0m\n",
      "    \u001b[0;31m[... skipping hidden 14 frame]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/jax-gpu/lib/python3.10/site-packages/jax/_src/compiler.py:256\u001b[0m, in \u001b[0;36mbackend_compile\u001b[0;34m(backend, module, options, host_callbacks)\u001b[0m\n\u001b[1;32m    251\u001b[0m   \u001b[39mreturn\u001b[39;00m backend\u001b[39m.\u001b[39mcompile(built_c, compile_options\u001b[39m=\u001b[39moptions,\n\u001b[1;32m    252\u001b[0m                          host_callbacks\u001b[39m=\u001b[39mhost_callbacks)\n\u001b[1;32m    253\u001b[0m \u001b[39m# Some backends don't have `host_callbacks` option yet\u001b[39;00m\n\u001b[1;32m    254\u001b[0m \u001b[39m# TODO(sharadmv): remove this fallback when all backends allow `compile`\u001b[39;00m\n\u001b[1;32m    255\u001b[0m \u001b[39m# to take in `host_callbacks`\u001b[39;00m\n\u001b[0;32m--> 256\u001b[0m \u001b[39mreturn\u001b[39;00m backend\u001b[39m.\u001b[39;49mcompile(built_c, compile_options\u001b[39m=\u001b[39;49moptions)\n",
      "\u001b[0;31mXlaRuntimeError\u001b[0m: INTERNAL: cuSolver internal error"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:jax._src.compiler:Not writing persistent cache entry for 'jit_jsoln_jax_ty_jit_quasisep' because it uses host callbacks (e.g. from jax.debug.print or breakpoint)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.96 s, sys: 179 ms, total: 8.14 s\n",
      "Wall time: 9.51 s\n"
     ]
    }
   ],
   "source": [
    "# doesnt work, maybe cuda problem \n",
    "\n",
    "%time res_jsoln_jax_ty_gpu = jsoln_jax_ty_gpu(t, y, yerr)\n",
    "%time res_jsoln_jax_ty_quasisep_gpu = jsoln_jax_ty_quasisep_gpu(t, y, yerr)\n",
    "%time res_jsoln_jax_ty_gpu = jsoln_jax_ty_jit_gpu(t, y, yerr)\n",
    "%time res_jsoln_jax_ty_quasisep_gpu = jsoln_jax_ty_jit_quasisep_gpu(t, y, yerr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:jax._src.dispatch:Finished tracing + transforming <lambda> for pjit in 0.0010919570922851562 sec\n",
      "DEBUG:jax._src.dispatch:Finished tracing + transforming <lambda> for pjit in 0.00042510032653808594 sec\n",
      "DEBUG:jax._src.dispatch:Finished tracing + transforming square for pjit in 0.0004372596740722656 sec\n",
      "DEBUG:jax._src.dispatch:Finished tracing + transforming _reduce_sum for pjit in 0.0007770061492919922 sec\n",
      "DEBUG:jax._src.dispatch:Finished tracing + transforming true_divide for pjit in 0.0005528926849365234 sec\n",
      "DEBUG:jax._src.dispatch:Finished tracing + transforming absolute for pjit in 0.00037169456481933594 sec\n",
      "DEBUG:jax._src.dispatch:Finished tracing + transforming fn for pjit in 0.0004432201385498047 sec\n",
      "DEBUG:jax._src.dispatch:Finished tracing + transforming _power for pjit in 0.00043702125549316406 sec\n",
      "DEBUG:jax._src.dispatch:Finished tracing + transforming fn for pjit in 0.0004482269287109375 sec\n",
      "DEBUG:jax._src.dispatch:Finished tracing + transforming _broadcast_arrays for pjit in 0.0002963542938232422 sec\n",
      "DEBUG:jax._src.dispatch:Finished tracing + transforming _squeeze for pjit in 0.0001652240753173828 sec\n",
      "DEBUG:jax._src.dispatch:Finished tracing + transforming tril for pjit in 0.0006549358367919922 sec\n",
      "DEBUG:jax._src.dispatch:Finished tracing + transforming _cholesky for pjit in 0.00159454345703125 sec\n",
      "DEBUG:jax._src.dispatch:Finished tracing + transforming fn for pjit in 0.00023365020751953125 sec\n",
      "DEBUG:jax._src.dispatch:Finished tracing + transforming true_divide for pjit in 0.0002338886260986328 sec\n",
      "DEBUG:jax._src.dispatch:Finished tracing + transforming _solve_triangular for pjit in 0.0008838176727294922 sec\n",
      "DEBUG:jax._src.dispatch:Finished tracing + transforming _get_alpha for pjit in 0.0019659996032714844 sec\n",
      "DEBUG:jax._src.dispatch:Finished tracing + transforming tril for pjit in 0.0005941390991210938 sec\n",
      "DEBUG:jax._src.dispatch:Finished tracing + transforming _moveaxis for pjit in 0.00012993812561035156 sec\n",
      "DEBUG:jax._src.dispatch:Finished tracing + transforming diagonal for pjit in 0.0028748512268066406 sec\n",
      "DEBUG:jax._src.dispatch:Finished tracing + transforming _diag for pjit in 0.0037415027618408203 sec\n",
      "DEBUG:jax._src.dispatch:Finished tracing + transforming _compute_log_prob for pjit in 0.006392478942871094 sec\n",
      "DEBUG:jax._src.dispatch:Finished tracing + transforming neg_log_likelihood for pjit in 0.21273422241210938 sec\n",
      "DEBUG:jax._src.interpreters.pxla:Compiling neg_log_likelihood for with global shapes and types [ShapedArray(float64[]), ShapedArray(float64[4]), ShapedArray(float64[]), ShapedArray(float64[]), ShapedArray(float64[]), ShapedArray(float64[4]), ShapedArray(float64[]), ShapedArray(float64[2222]), ShapedArray(float64[2222])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).\n",
      "DEBUG:jax._src.dispatch:Finished tracing + transforming fn for pjit in 0.00028443336486816406 sec\n",
      "DEBUG:jax._src.dispatch:Finished tracing + transforming _broadcast_arrays for pjit in 0.00017905235290527344 sec\n",
      "DEBUG:jax._src.dispatch:Finished tracing + transforming <lambda> for pjit in 0.00024080276489257812 sec\n",
      "DEBUG:jax._src.dispatch:Finished jaxpr to MLIR module conversion jit(neg_log_likelihood) in 0.05702400207519531 sec\n",
      "DEBUG:jax._src.compiler:get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[cuda(id=0)]]\n",
      "DEBUG:jax._src.compiler:get_compile_options XLA-AutoFDO profile: using XLA-AutoFDO profile version -1\n",
      "DEBUG:jax._src.cache_key:get_cache_key hash of serialized computation: 96f37120a1056ab98e9cf71cbf9a384325e63283a3b96075b94967367bc329aa\n",
      "DEBUG:jax._src.cache_key:get_cache_key hash after serializing computation: 96f37120a1056ab98e9cf71cbf9a384325e63283a3b96075b94967367bc329aa\n",
      "DEBUG:jax._src.cache_key:get_cache_key hash of serialized jax_lib version: 99b916eb3ced1f033fba99b2686e992e18db31b37c7123db6c79dd907e1dfd9e\n",
      "DEBUG:jax._src.cache_key:get_cache_key hash after serializing jax_lib version: 637aa71bcd8cf36fc45baa7bf44b95113e94175386b0b274a2882e3f2f6929ce\n",
      "DEBUG:jax._src.cache_key:get_cache_key hash of serialized XLA flags: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "DEBUG:jax._src.cache_key:get_cache_key hash after serializing XLA flags: 637aa71bcd8cf36fc45baa7bf44b95113e94175386b0b274a2882e3f2f6929ce\n",
      "DEBUG:jax._src.cache_key:get_cache_key hash of serialized compression: 0ea55c28f8014d8886b6248fe3da5d588f55c0823847a6b4579f1131b051b5e2\n",
      "DEBUG:jax._src.cache_key:get_cache_key hash after serializing compression: 36cd63c9261c57ec86f606c1362cf594938f8c5f9ab78d4f04c766d14dcf5201\n",
      "DEBUG:jax._src.cache_key:get_cache_key hash of serialized compile_options: 8536109503554a43dc92214c7524577150e8630911c4501102c97941e78672d0\n",
      "DEBUG:jax._src.cache_key:get_cache_key hash after serializing compile_options: e176a43853738de29e0e2362e0946b4162255b90dcffda05329a5104c069e37b\n",
      "DEBUG:jax._src.cache_key:get_cache_key hash of serialized devices: 2a25d4df60bc794265a0b3ac4c3e7af0b8de32bda498cef13b09ffe9e72c60c5\n",
      "DEBUG:jax._src.cache_key:get_cache_key hash after serializing devices: a5478a7528b3c4a59593db9f6ac7ba1c319cf04be3f033c7824875354e52f961\n",
      "DEBUG:jax._src.cache_key:get_cache_key hash of serialized the backend: 8bf80c9a211afbdabf117cb61bf4bc323baf92ba2c38c32749b1dc899bc8e121\n",
      "DEBUG:jax._src.cache_key:get_cache_key hash after serializing the backend: 1432ba09a28dfb75afb8368b9dc16914c7b46ffe782886b24025a3807fc25ec4\n",
      "DEBUG:jax._src.compiler:'jit_neg_log_likelihood' took at least 0.10 seconds to compile (1.99s), writing persistent cache entry\n",
      "INFO:jax._src.compilation_cache:Writing jit_neg_log_likelihood to persistent compilation cache with key jit_neg_log_likelihood-1432ba09a28dfb75afb8368b9dc16914c7b46ffe782886b24025a3807fc25ec4.\n",
      "DEBUG:jax._src.dispatch:Finished XLA compilation of jit(neg_log_likelihood) in 2.0461254119873047 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial negative log likelihood: 19692.68549568087\n",
      "Gradient of the negative log likelihood, wrt the parameters:\n",
      "{'log_alpha': Array(108.16638225, dtype=float64), 'log_amps': Array([-1.71192863e+04, -1.24699148e+02,  2.25648065e+02, -7.62560177e+00],      dtype=float64), 'log_diag': Array(402.97888208, dtype=float64), 'log_gamma': Array(266.50043559, dtype=float64), 'log_period': Array(-213.51481254, dtype=float64), 'log_scales': Array([-11937.8256461 ,   -356.5112549 ,   -552.36351147,   -115.72900226],      dtype=float64), 'mean': Array(104.33296606, dtype=float64)}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def build_gp(theta, X):\n",
    "    # We want most of our parameters to be positive so we take the `exp` here\n",
    "    # Note that we're using `jnp` instead of `np`\n",
    "    amps = jnp.exp(theta[\"log_amps\"])\n",
    "    scales = jnp.exp(theta[\"log_scales\"])\n",
    "\n",
    "    # Construct the kernel by multiplying and adding `Kernel` objects\n",
    "    k1 = amps[0] * kernels.ExpSquared(scales[0])\n",
    "    k2 = (\n",
    "        amps[1]\n",
    "        * kernels.ExpSquared(scales[1])\n",
    "        * kernels.ExpSineSquared(\n",
    "            scale=jnp.exp(theta[\"log_period\"]),\n",
    "            gamma=jnp.exp(theta[\"log_gamma\"]),\n",
    "        )\n",
    "    )\n",
    "    k3 = amps[2] * kernels.RationalQuadratic(\n",
    "        alpha=jnp.exp(theta[\"log_alpha\"]), scale=scales[2]\n",
    "    )\n",
    "    k4 = amps[3] * kernels.ExpSquared(scales[3])\n",
    "    kernel = k1 + k2 + k3 + k4\n",
    "\n",
    "    return GaussianProcess(\n",
    "        kernel, X, diag=jnp.exp(theta[\"log_diag\"]), mean=theta[\"mean\"]\n",
    "    )\n",
    "\n",
    "\n",
    "def neg_log_likelihood(theta, X, y):\n",
    "    gp = build_gp(theta, X)\n",
    "    return -gp.log_probability(y)\n",
    "\n",
    "\n",
    "theta_init = {\n",
    "    \"mean\": np.float64(340.0),\n",
    "    \"log_diag\": np.log(0.19),\n",
    "    \"log_amps\": np.log([66.0, 2.4, 0.66, 0.18]),\n",
    "    \"log_scales\": np.log([67.0, 90.0, 0.78, 1.6]),\n",
    "    \"log_period\": np.float64(0.0),\n",
    "    \"log_gamma\": np.log(4.3),\n",
    "    \"log_alpha\": np.log(1.2),\n",
    "}\n",
    "\n",
    "# `jax` can be used to differentiate functions, and also note that we're calling\n",
    "# `jax.jit` for the best performance.\n",
    "obj = jax.jit(jax.value_and_grad(neg_log_likelihood))\n",
    "\n",
    "print(f\"Initial negative log likelihood: {obj(theta_init, t, y)[0]}\")\n",
    "print(\n",
    "    f\"Gradient of the negative log likelihood, wrt the parameters:\\n{obj(theta_init, t, y)[1]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "amp = 0.2\n",
    "tau = 100\n",
    "DRW_kernel = DRW_term(np.log(amp), np.log(tau))\n",
    "t, y_drw, yerr = gpSimRand(DRW_kernel, 10, 365*10, 2222)\n",
    "\n",
    "# This adds periodic component to the drw process\n",
    "y = y_drw + 0.2* np.sin(t/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.config.update(\"jax_persistent_cache_min_compile_time_secs\", 0.1)\n",
    "jax.config.update(\"jax_compilation_cache_include_metadata_in_key\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:jax._src.dispatch:Finished tracing + transforming fn for pjit in 0.0006060600280761719 sec\n",
      "DEBUG:jax._src.dispatch:Finished tracing + transforming _reduce_sum for pjit in 0.0006413459777832031 sec\n",
      "DEBUG:jax._src.dispatch:Finished tracing + transforming neg_log_likelihood_float_test for pjit in 0.005285024642944336 sec\n",
      "DEBUG:jax._src.interpreters.pxla:Compiling neg_log_likelihood_float_test for with global shapes and types [ShapedArray(float64[2222]), ShapedArray(float64[2222]), ShapedArray(float64[2222])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).\n",
      "DEBUG:jax._src.dispatch:Finished jaxpr to MLIR module conversion jit(neg_log_likelihood_float_test) in 0.0034036636352539062 sec\n",
      "DEBUG:jax._src.compiler:get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[cuda(id=0)]]\n",
      "DEBUG:jax._src.compiler:get_compile_options XLA-AutoFDO profile: using XLA-AutoFDO profile version -1\n",
      "DEBUG:jax._src.cache_key:get_cache_key hash of serialized computation: e28f544883693d866e2de887c71bbc73c392794d0e103b83b02b2728c2ea5726\n",
      "DEBUG:jax._src.cache_key:get_cache_key hash after serializing computation: e28f544883693d866e2de887c71bbc73c392794d0e103b83b02b2728c2ea5726\n",
      "DEBUG:jax._src.cache_key:get_cache_key hash of serialized jax_lib version: 99b916eb3ced1f033fba99b2686e992e18db31b37c7123db6c79dd907e1dfd9e\n",
      "DEBUG:jax._src.cache_key:get_cache_key hash after serializing jax_lib version: 06c5f3676849f50a8c4c45d1510bd53f93175eaf4520fb607f8a19481ac5c0ed\n",
      "DEBUG:jax._src.cache_key:get_cache_key hash of serialized XLA flags: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "DEBUG:jax._src.cache_key:get_cache_key hash after serializing XLA flags: 06c5f3676849f50a8c4c45d1510bd53f93175eaf4520fb607f8a19481ac5c0ed\n",
      "DEBUG:jax._src.cache_key:get_cache_key hash of serialized compression: 0ea55c28f8014d8886b6248fe3da5d588f55c0823847a6b4579f1131b051b5e2\n",
      "DEBUG:jax._src.cache_key:get_cache_key hash after serializing compression: d272823143dafa18a4c1f14dd02f894ae611e715d6e5f061e26848b96063d3f4\n",
      "DEBUG:jax._src.cache_key:get_cache_key hash of serialized compile_options: 6fc57653a293e7afd0ff1c1a3f1f7e667a7ba9d597fc558968bf5baa7317de42\n",
      "DEBUG:jax._src.cache_key:get_cache_key hash after serializing compile_options: 26c88c93e0b60402d9b3193823bec433a26ffbb02129d777d30a09472986719f\n",
      "DEBUG:jax._src.cache_key:get_cache_key hash of serialized devices: 2a25d4df60bc794265a0b3ac4c3e7af0b8de32bda498cef13b09ffe9e72c60c5\n",
      "DEBUG:jax._src.cache_key:get_cache_key hash after serializing devices: 37904ac434e0da06588e30496d2b9ca4cb6658ddc04e6e9b8006bd523086a23b\n",
      "DEBUG:jax._src.cache_key:get_cache_key hash of serialized the backend: 8bf80c9a211afbdabf117cb61bf4bc323baf92ba2c38c32749b1dc899bc8e121\n",
      "DEBUG:jax._src.cache_key:get_cache_key hash after serializing the backend: 89c4058bfde7858a6cc0fc0852d07900830a18e8391fa2252bbf90d9908ca9ac\n",
      "INFO:jax._src.compiler:Persistent compilation cache hit for 'jit_neg_log_likelihood_float_test'\n",
      "DEBUG:jax._src.dispatch:Finished XLA compilation of jit(neg_log_likelihood_float_test) in 0.013720035552978516 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial negative log likelihood: -4047113.132524418\n"
     ]
    }
   ],
   "source": [
    "# what if I have a simpler function\n",
    "\n",
    "@jax.jit\n",
    "def neg_log_likelihood_float_test(theta_float, t, y, yerr):\n",
    "    test = t + y + yerr\n",
    "    return -jnp.sum(test)\n",
    "\n",
    "print(f\"Initial negative log likelihood: {neg_log_likelihood_float_test(theta_init_float, t, y, yerr)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:jax._src.dispatch:Finished tracing + transforming _reduce_sum for pjit in 0.0006961822509765625 sec\n",
      "DEBUG:jax._src.dispatch:Finished tracing + transforming _mean for pjit in 0.0024716854095458984 sec\n",
      "DEBUG:jax._src.dispatch:Finished tracing + transforming ravel for pjit in 0.0001938343048095703 sec\n",
      "DEBUG:jax._src.dispatch:Finished tracing + transforming append for pjit in 0.001699686050415039 sec\n",
      "DEBUG:jax._src.dispatch:Finished tracing + transforming matmul for pjit in 0.0008721351623535156 sec\n",
      "DEBUG:jax._src.dispatch:Finished tracing + transforming fn for pjit in 0.00038242340087890625 sec\n",
      "DEBUG:jax._src.dispatch:Finished tracing + transforming _reduce_sum for pjit in 0.0004951953887939453 sec\n",
      "DEBUG:jax._src.dispatch:Finished tracing + transforming dot for pjit in 0.0006043910980224609 sec\n",
      "DEBUG:jax._src.dispatch:Finished tracing + transforming __add__ for pjit in 0.0005941390991210938 sec\n",
      "DEBUG:jax._src.dispatch:Finished tracing + transforming fn for pjit in 0.0003800392150878906 sec\n",
      "DEBUG:jax._src.dispatch:Finished tracing + transforming outer for pjit in 0.0021092891693115234 sec\n",
      "DEBUG:jax._src.dispatch:Finished tracing + transforming matmul for pjit in 0.0006527900695800781 sec\n",
      "DEBUG:jax._src.dispatch:Finished tracing + transforming matmul for pjit in 0.0006432533264160156 sec\n",
      "DEBUG:jax._src.dispatch:Finished tracing + transforming matmul for pjit in 0.0007498264312744141 sec\n",
      "DEBUG:jax._src.dispatch:Finished tracing + transforming fn for pjit in 0.0003762245178222656 sec\n",
      "DEBUG:jax._src.dispatch:Finished tracing + transforming cholesky for pjit in 0.01360940933227539 sec\n",
      "DEBUG:jax._src.dispatch:Finished tracing + transforming <lambda> for pjit in 0.0003757476806640625 sec\n",
      "DEBUG:jax._src.dispatch:Finished tracing + transforming solve for pjit in 0.0044193267822265625 sec\n",
      "DEBUG:jax._src.dispatch:Finished tracing + transforming _get_alpha for pjit in 0.006616115570068359 sec\n",
      "DEBUG:jax._src.dispatch:Finished tracing + transforming square for pjit in 0.00026702880859375 sec\n",
      "DEBUG:jax._src.dispatch:Finished tracing + transforming <lambda> for pjit in 0.0002880096435546875 sec\n",
      "DEBUG:jax._src.dispatch:Finished tracing + transforming _compute_log_prob for pjit in 0.004629850387573242 sec\n",
      "DEBUG:jax._src.dispatch:Finished tracing + transforming neg_log_likelihood_drw for pjit in 0.049440860748291016 sec\n",
      "DEBUG:jax._src.interpreters.pxla:Compiling neg_log_likelihood_drw for with global shapes and types [ShapedArray(float64[]), ShapedArray(float64[]), ShapedArray(float64[2222]), ShapedArray(float64[2222]), ShapedArray(float64[2222])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).\n",
      "DEBUG:jax._src.dispatch:Finished jaxpr to MLIR module conversion jit(neg_log_likelihood_drw) in 0.04634356498718262 sec\n",
      "DEBUG:jax._src.compiler:get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[cuda(id=0)]]\n",
      "DEBUG:jax._src.compiler:get_compile_options XLA-AutoFDO profile: using XLA-AutoFDO profile version -1\n",
      "DEBUG:jax._src.cache_key:get_cache_key hash of serialized computation: 0a17cef32b9bcfa0af079f5862bbe8aa636d9854285923d20a1f8e1f7c054554\n",
      "DEBUG:jax._src.cache_key:get_cache_key hash after serializing computation: 0a17cef32b9bcfa0af079f5862bbe8aa636d9854285923d20a1f8e1f7c054554\n",
      "DEBUG:jax._src.cache_key:get_cache_key hash of serialized jax_lib version: 99b916eb3ced1f033fba99b2686e992e18db31b37c7123db6c79dd907e1dfd9e\n",
      "DEBUG:jax._src.cache_key:get_cache_key hash after serializing jax_lib version: a983cc5204d7311da19a5da57d4e482f7c09ea6803f94ceebb6379582b6b4cde\n",
      "DEBUG:jax._src.cache_key:get_cache_key hash of serialized XLA flags: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "DEBUG:jax._src.cache_key:get_cache_key hash after serializing XLA flags: a983cc5204d7311da19a5da57d4e482f7c09ea6803f94ceebb6379582b6b4cde\n",
      "DEBUG:jax._src.cache_key:get_cache_key hash of serialized compression: 0ea55c28f8014d8886b6248fe3da5d588f55c0823847a6b4579f1131b051b5e2\n",
      "DEBUG:jax._src.cache_key:get_cache_key hash after serializing compression: dbafb2d89a2d45b9dd60be8ffe8e9c8f8dc333f4512094ddbadd3fe20f88b6f0\n",
      "DEBUG:jax._src.cache_key:get_cache_key hash of serialized compile_options: 6fc57653a293e7afd0ff1c1a3f1f7e667a7ba9d597fc558968bf5baa7317de42\n",
      "DEBUG:jax._src.cache_key:get_cache_key hash after serializing compile_options: dffe834c2ea14911ce5178cb0b91b5ecb5da509dcdf0cffa02954292dfc26332\n",
      "DEBUG:jax._src.cache_key:get_cache_key hash of serialized devices: 2a25d4df60bc794265a0b3ac4c3e7af0b8de32bda498cef13b09ffe9e72c60c5\n",
      "DEBUG:jax._src.cache_key:get_cache_key hash after serializing devices: 12c1b2e5be69701f9db6644b9dcc650a6943a8598784571f31640878006ff74a\n",
      "DEBUG:jax._src.cache_key:get_cache_key hash of serialized the backend: 8bf80c9a211afbdabf117cb61bf4bc323baf92ba2c38c32749b1dc899bc8e121\n",
      "DEBUG:jax._src.cache_key:get_cache_key hash after serializing the backend: d44634ff89d4823b65998a2b0919d49cd1ebac3b2fcf02904b3a463ecf33958e\n",
      "INFO:jax._src.compiler:Not writing persistent cache entry for 'jit_neg_log_likelihood_drw' because it uses host callbacks (e.g. from jax.debug.print or breakpoint)\n",
      "DEBUG:jax._src.dispatch:Finished XLA compilation of jit(neg_log_likelihood_drw) in 0.4285876750946045 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial negative log likelihood: -1949.2229968120407\n"
     ]
    }
   ],
   "source": [
    "@jax.jit\n",
    "def neg_log_likelihood_drw(theta, t, y, yerr):\n",
    "    gp = build_exp_gp(theta, t, y, yerr)\n",
    "    return -gp.log_probability(y)\n",
    "\n",
    "\n",
    "print(f\"Initial negative log likelihood: {neg_log_likelihood_drw(theta_init, t, y, yerr)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "N = 10:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 't' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/astro/users/ncaplar/github/JaxPeriodDrwFit/Test_gpu.ipynb Cell 31\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbaldur/astro/users/ncaplar/github/JaxPeriodDrwFit/Test_gpu.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m variation_range \u001b[39m=\u001b[39m \u001b[39m0.0000001\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbaldur/astro/users/ncaplar/github/JaxPeriodDrwFit/Test_gpu.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_lc):  \u001b[39m# Change the number as per your requirement\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bbaldur/astro/users/ncaplar/github/JaxPeriodDrwFit/Test_gpu.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     t_var \u001b[39m=\u001b[39m t\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbaldur/astro/users/ncaplar/github/JaxPeriodDrwFit/Test_gpu.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m     y_var \u001b[39m=\u001b[39my\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbaldur/astro/users/ncaplar/github/JaxPeriodDrwFit/Test_gpu.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m     yerr_var \u001b[39m=\u001b[39m yerr\n",
      "\u001b[0;31mNameError\u001b[0m: name 't' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# generate fake data\n",
    "# how many lightcurves to create\n",
    "n_lc = 10\n",
    "print(f\"\\nN = {n_lc}:\")\n",
    "\n",
    "# Define the range for random variations|\n",
    "variation_range = 0.01  # Adjust this value to control the extent of variation\n",
    "\n",
    "t_batch = []\n",
    "y_batch = []\n",
    "yerr_batch = []\n",
    "\n",
    "# Create variations\n",
    "variation_range = 0.0000001\n",
    "for _ in range(n_lc):  # Change the number as per your requirement\n",
    "    t_var = t\n",
    "    y_var =y\n",
    "    yerr_var = yerr\n",
    "    t_batch.append(t_var)\n",
    "    y_batch.append(y_var)\n",
    "    yerr_batch.append(yerr_var)\n",
    "\n",
    "t_batch = np.array(t_batch)\n",
    "y_batch = np.array(y_batch)\n",
    "yerr_batch = np.array(yerr_batch)\n",
    "\n",
    "t_batch_jax = jnp.array(t_batch)\n",
    "y_batch_jax = jnp.array(y_batch)\n",
    "yerr_batch_jax = jnp.array(yerr_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:jax._src.interpreters.pxla:Compiling jsoln_jax_ty for with global shapes and types [ShapedArray(float64[10,203]), ShapedArray(float64[10,203]), ShapedArray(float64[10,203])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).\n",
      "DEBUG:jax._src.dispatch:Finished jaxpr to MLIR module conversion jit(jsoln_jax_ty) in 0.697577714920044 sec\n",
      "DEBUG:jax._src.compiler:get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[cuda(id=0)]]\n",
      "DEBUG:jax._src.compiler:get_compile_options XLA-AutoFDO profile: using XLA-AutoFDO profile version -1\n",
      "DEBUG:jax._src.cache_key:get_cache_key hash of serialized computation: 19294f348f7ebeea596452d60a258b965c2071d8558fbc11c76dd5d1921d21f7\n",
      "DEBUG:jax._src.cache_key:get_cache_key hash after serializing computation: 19294f348f7ebeea596452d60a258b965c2071d8558fbc11c76dd5d1921d21f7\n",
      "DEBUG:jax._src.cache_key:get_cache_key hash of serialized jax_lib version: 99b916eb3ced1f033fba99b2686e992e18db31b37c7123db6c79dd907e1dfd9e\n",
      "DEBUG:jax._src.cache_key:get_cache_key hash after serializing jax_lib version: 81795c6c6cff498b7bb07c99a346f720815c456a855ed670d881c97609189597\n",
      "DEBUG:jax._src.cache_key:get_cache_key hash of serialized XLA flags: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "DEBUG:jax._src.cache_key:get_cache_key hash after serializing XLA flags: 81795c6c6cff498b7bb07c99a346f720815c456a855ed670d881c97609189597\n",
      "DEBUG:jax._src.cache_key:get_cache_key hash of serialized compression: 0ea55c28f8014d8886b6248fe3da5d588f55c0823847a6b4579f1131b051b5e2\n",
      "DEBUG:jax._src.cache_key:get_cache_key hash after serializing compression: 11533aafe80d50cd716306050c4096c344b55a3877f09db30461f185d043ac96\n",
      "DEBUG:jax._src.cache_key:get_cache_key hash of serialized compile_options: 73d60c0a35e927de0bf96c765b04c0d7521120cd98571d35e63c0e63b5c4ee58\n",
      "DEBUG:jax._src.cache_key:get_cache_key hash after serializing compile_options: fab2980be71013691cd6f0602b365cf73621986935822c00a0dc8fde8d0bf6ce\n",
      "DEBUG:jax._src.cache_key:get_cache_key hash of serialized devices: 2a25d4df60bc794265a0b3ac4c3e7af0b8de32bda498cef13b09ffe9e72c60c5\n",
      "DEBUG:jax._src.cache_key:get_cache_key hash after serializing devices: e8b94c70b061b05ef05c528ee2da7ffefbbb47ede17d4ece8e97da1ef4339e59\n",
      "DEBUG:jax._src.cache_key:get_cache_key hash of serialized the backend: 8bf80c9a211afbdabf117cb61bf4bc323baf92ba2c38c32749b1dc899bc8e121\n",
      "DEBUG:jax._src.cache_key:get_cache_key hash after serializing the backend: aa361b4d23841055dcebd560e2cc666068b098ec619fbc954615cfe8e5cb8c33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:jax._src.compiler:Not writing persistent cache entry for 'jit_jsoln_jax_ty' because it uses host callbacks (e.g. from jax.debug.print or breakpoint)\n",
      "DEBUG:jax._src.dispatch:Finished XLA compilation of jit(jsoln_jax_ty) in 4.813591718673706 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.57 s, sys: 403 ms, total: 5.97 s\n",
      "Wall time: 6.95 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Array([-119.68832781, -119.68832781, -119.68832781, -119.68832781,\n",
       "       -119.68832781, -119.68832781, -119.68832781, -119.68832781,\n",
       "       -119.68832781, -119.68832781], dtype=float64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time jsoln_jax_ty_gpu_vmap(t_batch_jax, y_batch_jax, yerr_batch_jax).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tiny_lsst",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
